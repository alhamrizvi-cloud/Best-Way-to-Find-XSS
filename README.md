

# ğŸ”¥ Automated XSS Hunting Pipeline (Katana â†’ Dalfox)

This repository documents a **real-world automated workflow** for discovering **reflected XSS candidates** using modern recon and filtering tools.

> âš ï¸ Use **only on targets you are authorized to test**.


## ğŸ§° Tools Used

* **Katana** â€“ Web crawler
* **uro** â€“ URL normalizer & deduplicator
* **Gxss** â€“ Parameter injection helper
* **kxss** â€“ Reflection detector
* **Dalfox** â€“ Advanced XSS scanner


## ğŸ“Œ Step-by-Step Pipeline


### 1ï¸âƒ£ Crawl live subdomains with Katana

```bash
katana -list monzolive.txt -jc -kf all -d 3 -silent > katana_monzo.txt
```

**What this does:**

* `-list monzolive.txt` â†’ Input file containing live subdomains
* `-jc` â†’ Extract URLs from JavaScript
* `-kf all` â†’ Crawl all known file types
* `-d 3` â†’ Crawl depth (3 levels deep)
* `-silent` â†’ No banner, clean output

ğŸ“ **Output:** `katana_monzo.txt` (all discovered URLs)

### 2ï¸âƒ£ Extract URLs with parameters

```bash
grep "?" katana_monzo.txt > urls_with_params.txt
```

**Why:**

* XSS requires **parameters**
* This filters only URLs containing `?`

ğŸ“ **Output:** `urls_with_params.txt`

---

### 3ï¸âƒ£ Normalize & deduplicate URLs

```bash
cat urls_with_params.txt | uro > cleanparams.txt
```

**What uro does:**

* Removes duplicate parameters
* Normalizes URL structure
* Reduces massive noise

ğŸ“ **Output:** `cleanparams.txt`

---

### 4ï¸âƒ£ Inject XSS marker payload (Gxss)

```bash
cat cleanparams.txt | Gxss > gxss.txt
```

**Why Gxss:**

* Replaces parameter values with `Gxss`
* Helps detect reflection points

ğŸ“ **Output:** `gxss.txt`

---

### 5ï¸âƒ£ Detect reflected parameters (kxss)

```bash
cat gxss.txt | kxss > reflected.txt
```

**What kxss does:**

* Sends requests
* Detects **reflected parameters**
* Shows filtering behavior

ğŸ“ **Output:** `reflected.txt`

Example:

```
URL: https://target.com/search?q=Gxss
Param: q
Unfiltered: ['<', '>', '"']
```

---

### 6ï¸âƒ£ Extract only the vulnerable URLs

```bash
cat reflected.txt | grep -oP 'URL: \K\S+' > urls_only.txt
```

**Why:**

* Removes extra metadata
* Keeps only testable URLs

ğŸ“ **Output:** `urls_only.txt`

---

### 7ï¸âƒ£ Clean injected payload marker

```bash
sed 's/Gxss//g' urls_only.txt > urls_clean.txt
```

**Purpose:**

* Removes `Gxss` placeholder
* Prepares URLs for real payload testing

ğŸ“ **Output:** `urls_clean.txt`

---

### 8ï¸âƒ£ (Optional) Noise reduction

```bash
cat urls_clean.txt \
| grep -v "community.monzo.com/t/" \
| grep -v "page=" \
| sort -u \
> final_xss_targets.txt
```

**Why optional filtering:**

* Removes forum threads
* Removes pagination parameters
* Keeps high-value targets only

ğŸ“ **Final Targets:** `final_xss_targets.txt`

---

## ğŸš€ Final XSS Scanning with Dalfox

```bash
dalfox file final_xss_targets.txt \
  --custom-payload /home/alhamr/Downloads/xss_payloads.txt \
  --skip-mining-dom \
  --skip-mining-dict \
  --worker 50 \
  -o dalfox_results.txt
```

**Dalfox flags explained:**

* `file` â†’ Scan URLs from file
* `--custom-payload` â†’ Use your own payload list
* `--skip-mining-dom` â†’ Faster scan (no DOM mining)
* `--skip-mining-dict` â†’ Skip wordlist fuzzing
* `--worker 50` â†’ High concurrency
* `-o` â†’ Save results

ğŸ“ **Output:** `dalfox_results.txt`

```
sed 's/.*https/https/' dalfox_results.txt > cleandalfox.txt

```

## ğŸ§  Workflow Summary

```
Live Subdomains
      â†“
Katana Crawl
      â†“
Parameter Filtering
      â†“
URL Normalization
      â†“
Reflection Detection
      â†“
Noise Removal
      â†“
Dalfox Exploitation
```

---

## ğŸ¯ Why This Works

* Reduces false positives
* Focuses on **real reflection points**
* Uses **context-aware scanning**
* Matches **bug bounty methodologies**

---

## âš ï¸ Disclaimer

This project is for **educational and authorized testing only**.
The author is not responsible for misuse.

